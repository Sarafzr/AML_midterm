{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5a93a8-9f10-4815-bc63-715d9fcfef2a",
   "metadata": {
    "id": "LX0ia6JVtFjr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, SpectralClustering, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2f396b29-4411-486e-afc2-8c5832d1d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, loguniform\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36aafda-9267-4d82-a287-d8cb503e01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For handling warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1781c361-1f7f-4182-87f3-04ac5163c9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\utgoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\utgoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\utgoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\utgoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6119b8-c53f-4117-a5d7-931ad4d1afda",
   "metadata": {
    "id": "mD0dQabauB7z"
   },
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d41b9755-2f81-4b81-8206-5902c5ee28ff",
   "metadata": {
    "id": "uRBDrBxYtBbk"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/train.csv')#.dropna()\n",
    "val_data = pd.read_csv('./data/val.csv')#.dropna()\n",
    "test_data = pd.read_csv('./data/test.csv')#.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cefb83d3-79c1-46f4-a817-d8412b09be3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COVSCAfadeb2",
    "outputId": "98ef4bf3-943e-461c-e13c-4f6fe1fad5e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (59706,)\n",
      "Cleaned Train Data Shape: (24758,)\n",
      "Validation Data Shape: (23256,)\n",
      "Test Data Shape: (23257,)\n",
      " \n",
      "Number of labels = 0 in train dataset as percentage: 8.33%\n",
      "Number of labels = 1 in train dataset as percentage: 8.95%\n",
      "Number of labels = 2 in train dataset as percentage: 5.33%\n",
      "Number of labels = 3 in train dataset as percentage: 9.60%\n",
      "Number of labels = 4 in train dataset as percentage: 9.26%\n",
      "Number of labels = -100 in train dataset as percentage: 58.53%\n",
      " \n",
      "Number of labels = 0 in val dataset as percentage: 19.63%\n",
      "Number of labels = 1 in val dataset as percentage: 20.27%\n",
      "Number of labels = 2 in val dataset as percentage: 20.42%\n",
      "Number of labels = 3 in val dataset as percentage: 19.81%\n",
      "Number of labels = 4 in val dataset as percentage: 19.88%\n",
      "Number of labels = -100 in val dataset as percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# get all train data (labelled and unlabelled)\n",
    "X_train    = train_data['Phrase']\n",
    "y_train    = train_data['Sentiment']\n",
    "\n",
    "# get only labelled train data\n",
    "mask = (y_train != -100)\n",
    "train_data_clean    = train_data[mask]\n",
    "X_train_clean    = X_train[mask]\n",
    "y_train_clean    = y_train[mask]\n",
    "\n",
    "# get val data\n",
    "X_val    = val_data['Phrase']\n",
    "y_val    = val_data['Sentiment']\n",
    "\n",
    "# get test data\n",
    "X_test     = test_data['Phrase']\n",
    "\n",
    "print(f\"Train Data Shape: {X_train.shape}\")\n",
    "print(f\"Cleaned Train Data Shape: {train_data_clean['Phrase'].shape}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 0 in train dataset as percentage: {((y_train == 0).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 1 in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in train dataset as percentage: {((y_train == 2).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 3 in train dataset as percentage: {((y_train == 3).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 4 in train dataset as percentage: {((y_train == 4).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = -100 in train dataset as percentage: {((y_train == -100).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 0 in val dataset as percentage: {((y_val == 0).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 1 in val dataset as percentage: {((y_val == 1).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in val dataset as percentage: {((y_val == 2).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 3 in val dataset as percentage: {((y_val == 3).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 4 in val dataset as percentage: {((y_val == 4).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = -100 in val dataset as percentage: {((y_val == -100).sum() / (X_val.shape[0])) * 100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254f1ab-cf3c-4865-8734-7f275d1cdc7a",
   "metadata": {
    "id": "h_XKPAT4grMt"
   },
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d89f6a0-c5ff-4cdc-ad7e-db46af54debf",
   "metadata": {
    "id": "0Xlccu-qCz18"
   },
   "source": [
    "## Define Preprocessing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db81dade-4591-4d39-a6a7-a4576265f460",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5v2_Ro6ca5I",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "efd0ad1b-a203-4b8a-a8a0-36946906f7b7"
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', str(text), flags=re.MULTILINE)\n",
    "    texter = re.sub(r\"<br />\", \" \", text)\n",
    "    texter = re.sub(r\"&quot;\", \"\\\"\",texter)\n",
    "    texter = re.sub('&#39;', \"\\\"\", texter)\n",
    "    texter = re.sub('\\n', \" \", texter)\n",
    "    texter = re.sub(' u ',\" you \", texter)\n",
    "    texter = re.sub('`',\"\", texter)\n",
    "    texter = re.sub(' +', ' ', texter)\n",
    "    texter = re.sub(r\"(!)\\1+\", r\"!\", texter)\n",
    "    texter = re.sub(r\"(\\?)\\1+\", r\"?\", texter)\n",
    "    texter = re.sub('&amp;', 'and', texter)\n",
    "    texter = re.sub('\\r', ' ',texter)\n",
    "    #added substitutions\n",
    "\n",
    "    #***********added substitutions***********\n",
    "    # remove all the special characters\n",
    "    texter = re.sub(r'\\W', ' ', texter)\n",
    "    # remove all single characters\n",
    "    texter = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove single characters from the start\n",
    "    texter = re.sub(r'\\^[a-zA-Z]\\s+', ' ', texter)\n",
    "    # Remove numbers\n",
    "    texter = re.sub(r'\\d+', ' ', texter)\n",
    "    # Converting to Lowercase\n",
    "    texter = texter.lower()\n",
    "    # Remove punctuation\n",
    "    texter = re.sub(r'[^\\w\\s]', ' ', texter)\n",
    "    # Remove parentheses\n",
    "    texter = re.sub(r'\\([^)]*\\)', ' ', texter)\n",
    "    # Remove single quotes\n",
    "    texter = re.sub(r'\\'', ' ', texter)\n",
    "    # Substituting multiple spaces with single space\n",
    "    texter = re.sub(r'\\s+', ' ', texter, flags=re.I)\n",
    "\n",
    "    clean = re.compile('<.*?>')\n",
    "    texter = texter.encode('ascii', 'ignore').decode('ascii')\n",
    "    texter = re.sub(clean, '', texter)\n",
    "    if texter == \"\":\n",
    "        texter = \"\"\n",
    "    return texter\n",
    "\n",
    "def clean_dataset(dataset):\n",
    "    for row in range(dataset.shape[0]):\n",
    "        dataset[row,0] = clean(dataset[row,0])\n",
    "    return dataset\n",
    "\n",
    "def tokenize_lexicon(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(nltk.word_tokenize(texts[i]))\n",
    "        return_texts[i] = nltk.pos_tag(return_texts[i])\n",
    "    return return_texts\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "def lemmatize_texts(texts):\n",
    "    return_texts = []\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(lemmer.lemmatize(texts[i][j][0], pos=get_wordnet_pos(texts[i][j][1])))\n",
    "    return return_texts\n",
    "\n",
    "def stem_texts(texts):\n",
    "    return_texts = []\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append([])\n",
    "        for j in range(len(texts[i])):\n",
    "                return_texts[i].append(ps.stem(texts[i][j][0]))\n",
    "    return return_texts\n",
    "\n",
    "\n",
    "def backtostring(texts):\n",
    "    return_texts = []\n",
    "    for i in range(len(texts)):\n",
    "        return_texts.append(\" \".join(texts[i]))\n",
    "    return return_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3ace868-8482-44b0-ab26-1279b4840418",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWpMOdJ2uipq",
    "outputId": "426663c2-bce6-484a-9129-a6361491ca21"
   },
   "outputs": [],
   "source": [
    "def pre_process(data):\n",
    "    preproc_data = data.copy()\n",
    "    preproc_data = preproc_data.str.lower()\n",
    "    punctuation = string.punctuation\n",
    "    mapping = str.maketrans(\"\", \"\", punctuation)\n",
    "    preproc_data = preproc_data.str.translate(mapping)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([word for word in str(text).split() if word.lower() not in stop_words]))\n",
    "    # nltk.download('wordnet')\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # preproc_data = preproc_data.apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([stemmer.stem(word) for word in text.split()]))\n",
    "    \n",
    "    preproc_data = preproc_data.apply(lambda text: re.sub(r'@\\w+', '', re.sub(r'http\\S+|www\\S+', '', text)))\n",
    "    return preproc_data\n",
    "\n",
    "# get the preprocessed data\n",
    "X_train_preproc   = pre_process(X_train)\n",
    "X_train_clean_preproc   = pre_process(X_train_clean)\n",
    "X_val_preproc = pre_process(X_val)\n",
    "X_test_preproc = pre_process(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f3040-190c-4c6a-8735-3916e62fbcd2",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "cb9587f0-34d2-4ff0-9da5-a105a0286427",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "43bb3416-5e76-4194-9c48-d24abbaa3570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF feature matrix shape: (59706, 10456)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents='unicode', lowercase=True, tokenizer=tokenizer_porter, stop_words='english')\n",
    "X_train_preproc_tfidf = tfidf.fit_transform(X_train_preproc)\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_train_preproc_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "48737f26-cf4c-4300-bc86-76dfa6863967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF feature matrix shape: (23256, 10456)\n"
     ]
    }
   ],
   "source": [
    "X_val_preproc_tfidf = tfidf.transform(X_val_preproc)\n",
    "print(f\"\\nTF-IDF feature matrix shape: {X_val_preproc_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745dc4e-4634-4660-9d69-cd39961a17d5",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5270a88-4939-4ea6-9548-f55f62532e9a",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947059d5-1db9-4d28-8c7d-ff7a43ebf76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2400, random_state=42)  \n",
    "\n",
    "X_pca = pca.fit_transform(X_train_preproc_tfidf.toarray())\n",
    "print(f\"PCA-reduced feature matrix shape: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd6438-5971-4c32-9e08-ae8652d8cf18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sample_size = int(59701/5)\n",
    "k_values = [5, 50, 100, 250, 500, 750, 1000]  # Start from 2, as silhouette score needs at least two clusters\n",
    "bic_scores_pca = []\n",
    "aic_scores_pca = []\n",
    "\n",
    "# bic_scores_umap = []\n",
    "# aic_scores_umap = []\n",
    "\n",
    "# silhouette_scores_gmm = []\n",
    "\n",
    "for k in tqdm(k_values, desc=\"Finding K\"):\n",
    "\n",
    "    sample_indices = np.random.choice(X_pca.shape[0], size=sample_size, replace=False)\n",
    "    \n",
    "    # Fit a Gaussian Mixture Model (GMM) with k components\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type='diag', max_iter=100, reg_covar=1e-4, init_params='kmeans', random_state=42)\n",
    "    gmm_labels = gmm.fit_predict(X_pca[sample_indices])\n",
    "\n",
    "    # Calculate BIC and AIC\n",
    "    bic_scores_pca.append(gmm.bic(X_pca))\n",
    "    aic_scores_pca.append(gmm.aic(X_pca))\n",
    "\n",
    "    # gmm = GaussianMixture(n_components=k, covariance_type='diag', max_iter=100, reg_covar=1e-4, init_params='kmeans', random_state=42)\n",
    "    # gmm_labels = gmm.fit_predict(X_umap[sample_indices])\n",
    "    # # Calculate BIC and AIC\n",
    "    # bic_scores_umap.append(gmm.bic(X_umap))\n",
    "    # aic_scores_umap.append(gmm.aic(X_umap))\n",
    "\n",
    "    # print(bic_scores_umap)\n",
    "    # print(aic_scores_umap)\n",
    "\n",
    "    # Calculate Silhouette score if k > 1 (Silhouette requires at least 2 clusters)\n",
    "    # score = silhouette_score(X_pca, gmm_labels)\n",
    "    # silhouette_scores_gmm.append(score)\n",
    "\n",
    "# bic_scores_pca = [-10046903.765079908, -11768516.826057164, -12254994.691571359, -12697607.571135331, -12841542.659642098, -12771479.471291933, -12629629.958835967]\n",
    "# aic_scores_pca = [-10051438.305520937, -11813943.204403894, -12345856.445368867, -12924775.451285176, -13295887.417045837, -13453001.105949566, -13538328.470747493]\n",
    "\n",
    "# bic_scores_umap = [-679675.1635472373, -5523632.20674975, -7242640.424716241, -9202283.198420955, -10315246.580855194, -11068146.122167017, -11940543.615146823]\n",
    "# aic_scores_umap = [-684209.7039882655, -5569058.585096479, -7333502.178513749, -9429451.0785708, -10769591.338258933, -11749667.75682465, -12849242.12705835]\n",
    "\n",
    "# Plot the metrics for GMM\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot BIC and AIC scores\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(k_values, bic_scores_umap, marker='o', label='BIC-UMAP')\n",
    "# plt.plot(k_values, aic_scores_umap, marker='o',  label='AIC-UMAP')\n",
    "# plt.xticks(k_values)\n",
    "# plt.xlabel('Number of Components (k)')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('UMAP Scores')\n",
    "# plt.legend()\n",
    "\n",
    "# Plot the Silhouette Scores for GMM\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(k_values, bic_scores_pca, marker='o', label='BIC-PCA')\n",
    "plt.plot(k_values, aic_scores_pca, marker='o',  label='AIC-PCA')\n",
    "plt.xticks(k_values)\n",
    "plt.xlabel('Number of Components (k)')\n",
    "plt.ylabel('Score')\n",
    "plt.title('PCA Scores')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56a196e-624f-4c17-8b3b-e20623a0b62b",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a22f0-5d1c-47f2-bd3b-05937ac6ee45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def assign_labels_with_clustering(df, X, clustering_model, model_name):\n",
    "    \"\"\"\n",
    "    Assign labels to missing data based on clustering.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with 'Sentiment' column\n",
    "    - X: Feature matrix after PCA\n",
    "    - clustering_model: sklearn clustering model\n",
    "    - model_name: String name of the clustering model\n",
    "\n",
    "    Returns:\n",
    "    - df_combined: DataFrame with assigned labels\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {model_name} Clustering ---\")\n",
    "    # Fit the clustering model\n",
    "    clustering_model.fit(X)\n",
    "    \n",
    "    # Obtain cluster labels\n",
    "    if hasattr(clustering_model, 'labels_'):\n",
    "        clusters = clustering_model.labels_\n",
    "    else:\n",
    "        clusters = clustering_model.predict(X)\n",
    "    \n",
    "    df['Cluster'] = clusters\n",
    "   \n",
    "    # Debug: Show cluster distribution\n",
    "    print(f\"Number of clusters formed: {len(np.unique(clusters))}\")\n",
    "    # print(f\"Cluster distribution:\\n{pd.Series(clusters).value_counts()}\")\n",
    "    \n",
    "    # Map each cluster to the most frequent sentiment label within that cluster\n",
    "    cluster_label_map = {}\n",
    "    for cluster in np.unique(clusters):\n",
    "        # Select rows in the current cluster with known labels\n",
    "        mask = (df['Cluster'] == cluster) & (df['Sentiment'].notnull())\n",
    "        if mask.sum() == 0:\n",
    "            # If no labeled data in cluster, assign the most common overall label\n",
    "            most_common_label = df['Sentiment'].mode()[0]\n",
    "            cluster_label_map[cluster] = most_common_label\n",
    "            print(f\"Cluster {cluster}: No labeled data. Assigning most common label {most_common_label}.\")\n",
    "        else:\n",
    "            # Assign the most common label within the cluster           \n",
    "            most_common_label = df.loc[mask, 'Sentiment'].mode()[0]\n",
    "            cluster_label_map[cluster] = most_common_label\n",
    "            print(f\"Cluster {cluster}: Assigning label {most_common_label} based on majority voting.\")\n",
    "    \n",
    "    # Assign labels to missing data\n",
    "    def assign_label(row):\n",
    "        if pd.isnull(row['Sentiment']):\n",
    "            return cluster_label_map[row['Cluster']]\n",
    "        else:\n",
    "            return row['Sentiment']\n",
    "    \n",
    "    df['Sentiment_Assigned'] = df.apply(assign_label, axis=1)\n",
    "    \n",
    "    # Display the mapping\n",
    "    # print(f\"\\nCluster to Label Mapping for {model_name}:\")\n",
    "    # for cluster, label in cluster_label_map.items():\n",
    "    #     print(f\"Cluster {cluster}: Label {label}\")\n",
    "    \n",
    "    # Debug: Check the number of assigned labels\n",
    "    print(\"\\nLabel Distribution After Assignment:\")\n",
    "    print(df['Sentiment_Assigned'].value_counts())\n",
    "    \n",
    "    # Drop the 'Cluster' column as it's no longer needed\n",
    "    # df = df.drop('Cluster', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d1796e-f2e2-49c4-834f-a5f24082af38",
   "metadata": {},
   "source": [
    "## PCA-GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbdf25-595e-48db-8e83-223d92528be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=100, random_state=42)\n",
    "\n",
    "df_train = train_data.copy()\n",
    "df_train['Sentiment'] = np.where(df_train['Sentiment']==-100, np.nan, df_train['Sentiment'])\n",
    "\n",
    "df_gmm = assign_labels_with_clustering(df_train, X_pca, gmm, 'Gaussian Mixture Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9012a56e-e94d-482a-a7d5-79689b295e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gmm.to_csv('./data/best_clusters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c13e8-99c5-4a75-8d50-f301a735d542",
   "metadata": {},
   "source": [
    "# Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c8eb6051-9f80-4751-baae-53eee88d810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val = X_train_preproc_tfidf, df_gmm['Sentiment_Assigned'], X_val_preproc_tfidf, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e416feae-fbe6-43fd-b592-44c9dba2972a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'C': loguniform(1e-4, 1e4),\n",
    "        'penalty': ['l2', 'none'],  # l2 regularization or no regularization\n",
    "        'solver': ['lbfgs', 'liblinear', 'saga']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [None, 5],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [5, 50],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': loguniform(1e-4, 1e-1),\n",
    "        'num_leaves': [50, 70, 100],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_child_samples': [10, 20, 30],\n",
    "        'subsample': uniform(0.6, 0.4),  # sampling between 0.6 and 1.0\n",
    "        'colsample_bytree': uniform(0.6, 0.4), \n",
    "        \n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': [1,5,10,100, 200, 500],\n",
    "        'learning_rate': loguniform(1e-4, 1e-1),\n",
    "        'max_depth': [3, 6, 9, 12],\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'gamma': [0, 0.1, 0.2, 0.3],\n",
    "        'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "        'reg_lambda': [1, 1.5, 2, 3]\n",
    "    },\n",
    "    # 'multinomial_nb': {\n",
    "    #     'alpha': loguniform(1e-3, 1e0)  # Smoothing parameter\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(n_jobs=-1,),\n",
    "    'random_forest': RandomForestClassifier(random_state=42, n_jobs=-1,),\n",
    "    'lightgbm': lgb.LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1, verbose_eval=False,),\n",
    "    'xgboost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1,),\n",
    "    # 'multinomial_nb': MultinomialNB(n_jobs=-1,)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ada45-cb37-4d84-bafc-6fa951e3edd2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store best models and their performance metrics\n",
    "best_models = {}\n",
    "metrics = {}\n",
    "\n",
    "# Hyperparameter tuning using RandomizedSearchCV\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Tuning {model_name}...\")\n",
    "    # Use RandomizedSearchCV with 20 iterations and 3-fold cross-validation\n",
    "    random_search = RandomizedSearchCV(model, param_distributions=param_grids[model_name], \n",
    "                                       n_iter=20, scoring='accuracy', cv=3, n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = random_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {random_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd6cdc-79cd-4dab-b3c9-6ed939a3d899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_models['logistic_regression'] = {'C': 1.6, 'penalty': 'l2', 'solver': 'liblinear', 'n_jobs':-1}\n",
    "# best_models['random_forest'] = {'n_jobs':-1,'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'max_depth': None}\n",
    "# best_models['lightgbm'] = {'colsample_bytree': 0.839, 'learning_rate': 0.058, 'max_depth': 20, 'min_child_samples': 20, 'n_estimators': 500, 'num_leaves': 50, 'subsample': 0.985, 'n_jobs':-1, 'verbose': -1, 'verbose_eval':False,}\n",
    "# best_models['xgboost'] = {'n_jobs':-1, 'colsample_bytree': 0.78, 'gamma': 0.1, 'learning_rate': 0.048, 'max_depth': 6, 'n_estimators': 500, 'reg_alpha': 0.5, 'reg_lambda': 3, 'subsample': 0.618}\n",
    "# best_models['multinomial_nb'] = {'n_jobs':-1, \n",
    "\n",
    "# models = {\n",
    "#     'logistic_regression': LogisticRegression(**best_models['logistic_regression']),\n",
    "#     'random_forest': RandomForestClassifier(**best_models['random_forest']),\n",
    "#     'lightgbm': lgb.LGBMClassifier(**best_models['lightgbm']),\n",
    "#     'xgboost': xgb.XGBClassifier(**best_models['xgboost'])\n",
    "#     'multinomial_nb': MultinomialNB(**best_models['multinomial_nb'])\n",
    "# }\n",
    "\n",
    "# Evaluate each best model on the validation set\n",
    "for model_name, best_model in models.items():\n",
    "    print(f\"Best parameters for {model_name}: {best_models[model_name]}\")\n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    # y_pred_proba = best_model.predict_proba(X_val)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
    "    mcc = matthews_corrcoef(y_val, y_pred)\n",
    "    kappa = cohen_kappa_score(y_val, y_pred)\n",
    "\n",
    "    # roc_auc = roc_auc_score(y_val, y_pred_proba) if y_pred_proba is not None else \"N/A\"\n",
    "    \n",
    "    metrics[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'matthews_corrcoef':mcc,\n",
    "        'cohen_kappa':kappa\n",
    "        # 'roc_auc': roc_auc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2a7f6ed4-6baa-41d5-802d-bdcec5f82566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Performance of Best Models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>matthews_corrcoef</th>\n",
       "      <th>cohen_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.816</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm</th>\n",
       "      <td>0.847</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.858</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accuracy  f1_score  matthews_corrcoef  cohen_kappa\n",
       "xgboost                 0.816     0.817              0.772        0.770\n",
       "lightgbm                0.847     0.847              0.809        0.808\n",
       "random_forest           0.858     0.858              0.822        0.822\n",
       "logistic_regression     0.866     0.866              0.833        0.833"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the performance of each model\n",
    "metrics_df = pd.DataFrame(metrics).T.sort_values('accuracy').round(3)\n",
    "print(\"\\nValidation Performance of Best Models:\")\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede15de2-93d8-4c00-826e-f2a382db6ce3",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f746fa09-f16e-4443-93b7-35fb57787574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Define the explained variance threshold\n",
    "explained_variance_threshold = 0.9\n",
    "\n",
    "# Fit PCA without specifying the number of components\n",
    "pca = PCA().fit(X_train_preproc_tfidf.toarray())\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Find the number of components needed to reach the variance threshold\n",
    "n_components_optimal = np.argmax(cumulative_variance >= explained_variance_threshold) + 1\n",
    "print(f\"Optimal number of PCA components for {explained_variance_threshold * 100}% variance: {n_components_optimal}\")\n",
    "\n",
    "# Reduce the dataset to the optimal number of components\n",
    "# X_reduced = PCA(n_components=n_components_optimal).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ba050-d361-4b6c-851d-6dd293362d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot the cumulative explained variance\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), len(cumulative_variance)*[explained_variance_threshold], marker='_')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance by Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6456b168-09fc-4a67-b9ef-5dbe6e16a648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdd3f6e-8371-42e5-962c-b4c2efaf46e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
